{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch \n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "es=Elasticsearch([{'host':'127.0.0.1','port':9200}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_screen_name(index, name, es):\n",
    "    res= es.search(index=index,body=\n",
    "    {\n",
    "      \"query\": {\n",
    "        \"match\": {\n",
    "          \"screen_name\": name\n",
    "        }\n",
    "      }\n",
    "    })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = es.search(\n",
    "            index=config['elasticsearch_indices']['usertweets']['name'],\n",
    "            body={'query': {'match': {'user.screen_name': username}}}\n",
    "        )\n",
    "\n",
    "        hits = res['hits']['hits']\n",
    "\n",
    "        body = {\n",
    "            # data extracted from tweets\n",
    "            'full_text': [r['_source']['full_text'] for r in hits],\n",
    "            'hashtags': [[h['text']\n",
    "                    for h in r['_source']['entities']['hashtags']]\n",
    "                    for r in hits if r['_source']['entities']['hashtags']],\n",
    "            # basic user data\n",
    "            'location': hits[0]['_source']['user']['location'],\n",
    "            'description': hits[0]['_source']['user']['description'],\n",
    "            'screen_name': username,\n",
    "            'name': hits[0]['_source']['user']['name']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = search_by_screen_name(\"users\", \"sterling7\", es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_list(l):\n",
    "    return  [item for sublist in l for item in sublist]\n",
    "\n",
    "def common_tokens(tokens, n=20):\n",
    "    sentences = (list(itertools.chain(tokens)))\n",
    "    flat_sentences = flat_list(sentences)\n",
    "    counts = Counter(flat_sentences)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokens=[]\n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "\n",
    "for text in (res[\"hits\"][\"hits\"][0][\"_source\"][\"full_text\"]):\n",
    "    tokens_clean = []\n",
    "    for word in word_tokenize(text):\n",
    "        if word.lower() not in stop_words and word.lower() not in punctuation and not word.isnumeric() and len(word)> 1:\n",
    "            tokens_clean.append(word.lower())\n",
    "    list_tokens.append(tokens_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = common_tokens(list_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for k,v in counts.most_common(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = res[\"hits\"][\"hits\"][0][\"_source\"][\"hashtags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashcounts = common_tokens(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashcounts.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for k,v in hashcounts.most_common(30)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
