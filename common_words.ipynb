{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "twenty-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 10.0.1, however version 21.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip\n",
      "Requirement already satisfied: click in c:\\users\\gabriele.ferrario\\desktop\\information_retrieval\\progetto\\ir-project-personalized-search-engine\\venv\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/34/5b/bd0f0fb5564183884d8e35b81d06d7ec06a20d1a0c8b4c407f1554691dce/joblib-1.0.0-py3-none-any.whl\n",
      "Collecting regex (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/3f/40c8db23e022ccc9eb9fc0f39202af49c8614b22990b2e7129c2543f2da5/regex-2020.11.13-cp37-cp37m-win_amd64.whl\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/80/02/8f8880a4fd6625461833abcf679d4c12a44c76f9925f92bf212bb6cefaad/tqdm-4.56.0-py2.py3-none-any.whl\n",
      "Installing collected packages: joblib, regex, tqdm, nltk\n",
      "  Running setup.py install for nltk: started\n",
      "    Running setup.py install for nltk: finished with status 'done'\n",
      "Successfully installed joblib-1.0.0 nltk-3.5 regex-2020.11.13 tqdm-4.56.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coastal-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch \n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "infectious-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "es=Elasticsearch([{'host':'127.0.0.1','port':9200}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aquatic-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_screen_name(index, name, es):\n",
    "    res= es.search(index=index,body=\n",
    "    {\n",
    "      \"query\": {\n",
    "        \"match\": {\n",
    "          \"screen_name\": name\n",
    "        }\n",
    "      }\n",
    "    })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "primary-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = search_by_screen_name(\"users\", \"BarackObama\", es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handy-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_list(l):\n",
    "    return  [item for sublist in l for item in sublist]\n",
    "\n",
    "def common_tokens(tokens, n=20):\n",
    "    sentences = (list(itertools.chain(tokens)))\n",
    "    flat_sentences = flat_list(sentences)\n",
    "    counts = Counter(flat_sentences)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "swiss-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokens=[]\n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "\n",
    "for text in (res[\"hits\"][\"hits\"][0][\"_source\"][\"full_text\"]):\n",
    "    tokens_clean = []\n",
    "    for word in word_tokenize(text):\n",
    "        if word.lower() not in stop_words and word.lower() not in punctuation and not word.isnumeric() and len(word)> 1:\n",
    "            tokens_clean.append(word.lower())\n",
    "    list_tokens.append(tokens_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "above-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = common_tokens(list_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sonic-former",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https', 5),\n",
       " ('day', 4),\n",
       " ('one', 3),\n",
       " ('aaron', 2),\n",
       " ('ever', 2),\n",
       " ('people', 2),\n",
       " ('met', 2),\n",
       " ('climate', 2),\n",
       " ('today', 2),\n",
       " ('like', 2),\n",
       " (\"'re\", 2),\n",
       " ('brave', 2),\n",
       " ('enough', 2),\n",
       " ('hank', 1),\n",
       " ('best', 1),\n",
       " ('baseball', 1),\n",
       " ('players', 1),\n",
       " ('seen', 1),\n",
       " ('strongest', 1),\n",
       " ('michelle', 1),\n",
       " ('send', 1),\n",
       " ('thoughts', 1),\n",
       " ('prayers', 1),\n",
       " ('family', 1),\n",
       " ('everyone', 1),\n",
       " ('inspired', 1),\n",
       " ('unassuming', 1),\n",
       " ('man', 1),\n",
       " ('towering', 1),\n",
       " ('example', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "personal-career",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 15),\n",
       " ('and', 10),\n",
       " ('the', 9),\n",
       " (':', 6),\n",
       " (',', 6),\n",
       " ('you', 6),\n",
       " ('https', 5),\n",
       " ('a', 5),\n",
       " ('was', 4),\n",
       " ('we', 4),\n",
       " ('is', 4),\n",
       " ('for', 4),\n",
       " ('day', 4),\n",
       " ('only', 4),\n",
       " ('because', 4),\n",
       " ('one', 3),\n",
       " ('of', 3),\n",
       " ('our', 3),\n",
       " ('to', 3),\n",
       " ('this', 3),\n",
       " ('that', 3),\n",
       " ('it', 3),\n",
       " ('aaron', 2),\n",
       " ('â€™', 2),\n",
       " ('ve', 2),\n",
       " ('ever', 2),\n",
       " ('people', 2),\n",
       " ('i', 2),\n",
       " ('met', 2),\n",
       " ('by', 2)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-dodge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
